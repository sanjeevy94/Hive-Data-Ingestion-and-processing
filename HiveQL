create database sanjeevy94;
-- Task 1. environment setup with pre-requisite commands
ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-hcatalog-core-1.1.0-cdh5.11.2.jar;

-- for partitions
SET hive.exec.max.dynamic.partitions=100000;
SET hive.exec.max.dynamic.partitions.pernode=100000;

-- Task 2. Create Database and add data

Create database if not exists  ny_taxi_assignment_sa;
use  ny_taxi_assignment_sa;
-- create table
create external table if not exists main_data_ny(
VendorID int,
tpep_pickup_datetime timestamp,
tpep_dropoff_datetime timestamp,
passenger_count int,
trip_distance double,
RatecodeID int,
store_and_fwd_flag string,
PULocationID int,
DOLocationID int,
payment_type int,
fare_amount double,
extra double,
mta_tax double,
tip_amount double,
tolls_amount double,
improvement_surcharge double,
total_amount double
)

ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/common_folder/nyc_taxi_data/'
tblproperties ("skip.header.line.count"="2");
-- skipping the header and blank row

-- querying table for validation
select * from main_data_ny limit 10;
select count(*) from main_data_ny;
--Total records :-  1174568 record

-- Basic Data Quality Checks
-- 1.How many records has each TPEP provider provided? 
-- query for the number of records of each provider.

select vendorid,count(*) from main_data_ny
group by vendorid;
-- Result:- Vender 1 : 647183 Records
-- Result : Vender 2 : 527385 Records

select 647183/1174568 as ratio;
-- 55 % data is from vender 2 and rest is from vender 1

-- Checking Empty cells

select sum(case when VendorID is null then 1 else 0 end) VendorID,
sum(case when tpep_pickup_datetime is null then 1 else 0 end)	tpep_pickup_datetime,
sum(case when tpep_dropoff_datetime is null then 1 else 0 end) tpep_dropoff_datetime,
sum(case when passenger_count is null then 1 else 0 end) passenger_count,
sum(case when trip_distance is null then 1 else 0 end) trip_distance,
sum(case when RatecodeID is null then 1 else 0 end) RatecodeID,
sum(case when store_and_fwd_flag is null then 1 else 0 end) store_and_fwd_flag,
sum(case when PULocationID is null then 1 else 0 end) PULocationID,
sum(case when DOLocationID is null then 1 else 0 end) DOLocationID,
sum(case when payment_type is null then 1 else 0 end) payment_type,
sum(case when fare_amount is null then 1 else 0 end) fare_amount,
sum(case when extra is null then 1 else 0 end) extra,
sum(case when mta_tax is null then 1 else 0 end) mta_tax,
sum(case when tip_amount is null then 1 else 0 end) tip_amount,
sum(case when tolls_amount is null then 1 else 0 end)	tolls_amount,
sum(case when improvement_surcharge is null then 1 else 0 end)	improvement_surcharge,
sum(case when total_amount is null then 1 else 0 end) total_amount 	
from main_data_ny;

--  All results as zero so we have no empty cells


-- let's check the data range for all columns 
select max(VendorID ) max_VendorID, min(VendorID) min_VendorID,
max(tpep_pickup_datetime) max_tpep_pickup_datetime,	min(tpep_pickup_datetime) min_tpep_pickup_datetime,
max(tpep_dropoff_datetime) max_tpep_dropoff_datetime, min(tpep_dropoff_datetime) min_tpep_dropoff_datetime,
max(passenger_count) max_passenger_count, min(passenger_count) min_passenger_count,
max(trip_distance) max_trip_distance, min(trip_distance) min_trip_distance,
max(RatecodeID) max_RatecodeID,	min(RatecodeID) min_RatecodeID,
max(store_and_fwd_flag) max_store_and_fwd_flag, min(store_and_fwd_flag) min_store_and_fwd_flag,
max(PULocationID) max_PULocationID,	min(PULocationID) min_PULocationID,
max(DOLocationID) max_DOLocationID,	min(DOLocationID) min_DOLocationID,
max(payment_type) max_payment_type,	min(payment_type) min_payment_type,
max(fare_amount) max_fare_amount, min(fare_amount) min_fare_amount,
max(extra) max_extra, min(extra) min_extra,
max(mta_tax) max_mta_tax, min(mta_tax) min_mta_tax,
max(tip_amount) max_tip_amount, min(tip_amount) min_tip_amount,
max(tolls_amount ) max_tolls_amount, min(tolls_amount) min_tolls_amount,
max(improvement_surcharge) max_improvement_surcharge, min(improvement_surcharge) min_improvement_surcharge,
max(total_amount ) max_total_amount, min(total_amount) min_total_amount		
from main_data_ny;

-- column which seems to match the data disctionary 
-- pulocationid,dolocationid:- pickup and drop location are raning from 1 to 265
-- payment_type in data is spread between 1-4 which is with provided values of 1-6
-- vendorid is fine; values between two provider of 1 & 2

-- 2. The data provided is for months November and December only. 
--  tpep_pickup_datetime check
select count(*) from  main_data_ny 
where tpep_pickup_datetime < '2017-11-1 00:00:00.0' or tpep_pickup_datetime>='2018-01-01 00:00:00.0';

-- There are 14 records are not in range 
-- Let's find out which vender provide the incorrect data

select  vendorid, count(*)from  main_data_ny 
where tpep_pickup_datetime < '2017-11-1 00:00:00.0' or tpep_pickup_datetime>='2018-01-01 00:00:00.0'
group by vendorid;

-- Result :- 2. So vender 2 gives the faulty data
--  tpep_dropoff_datetime
--  As drop may happen on the next day so drop time is allowed till 01 Jan 2018

select count(*) from main_data_ny 
where tpep_dropoff_datetime < '2017-11-1 00:00:00.0' or tpep_dropoff_datetime>='2018-01-02 00:00:00.0';

-- There are 7 records are not in range 
-- Let's find out which vender provide the incorrect data

select vendorid,count(*) from main_data_ny  
where tpep_dropoff_datetime < '2017-11-1 00:00:00.0' or tpep_dropoff_datetime>='2018-01-02 00:00:00.0'
group by vendorid;

-- 6 faulty records out of 7 are from vender 2 and 1 faulty record from vender 1
-- As there are only 7 records out of range so let's explore these records.comment

select * from main_data_ny
where (tpep_dropoff_datetime < '2017-11-1 00:00:00.0' or tpep_dropoff_datetime>='2018-01-02 00:00:00.0');

-- All records from vender 2 are from older but the record from vender pickup in the range but drop on 24/04/2019 and it seems the data is corrupt

-- pick up time can't be greater than or equal to the drop of time

select count(*) from main_data_ny
where tpep_dropoff_datetime<=tpep_pickup_datetime;

--  There are 6555 records which are incorrect/corrupted
--  these are a smaller set of total records and can be deleted or ignored
-- Let's find out which vender provide this faulty data

select vendorid, count(*) from main_data_ny
where tpep_dropoff_datetime <= tpep_pickup_datetime
group by vendorid;
-- Vender 2 provide 3063 and vender 1 gives 3492 faulty records
-- Let's look some of these records 

select * from main_data_ny
where tpep_dropoff_datetime <= tpep_pickup_datetime limit 10;

-- Location id and total Amount varies a lot and it's a small set of records we can ignore them

-- Passenger count 
select passenger_count, count(*) from main_data_ny group by passenger_count;

-- There are 6824 records which have 0 as passenger count, it seems either some parcel was sent or driver didn't fil the reocrd properly
-- 9 passenger doesn't seem appropriate but it's only 1 record and we can ignore it.

-- Let's see which vendor is at fault here
select vendorid,passenger_count, count(*) from main_data_ny
where passenger_count in  (0,7,8,9) group by vendorid,passenger_count
order by passenger_count,vendorid;

-- It's clear that vender 1 have much error as compared to the vender 2. We can ignore the data with 0 count and rest can be assumed as bigger cars

-- trip_distance

select max(trip_distance) max_trip_distance, min(trip_distance) min_trip_distance from main_data_ny;

-- Max trip distance is 126.41 and minimum is 0

select count(*) from main_data_ny where trip_distance<=0;

--  7402 records have trip distance = 0
-- This is a small set of records and we can ignore or delete this

select  vendorid,count(*) from  main_data_ny where trip_distance<=0 group by vendorid;
-- 1 4217
-- 2 3185
-- Venfer 1 gives more faulty data than Vender 2

-- ## ratecodeid
select  ratecodeid,count(*) from  main_data_ny group by ratecodeid;
-- 1-6 are valid id as per teh metadata, and 9 records having 99 in value are incorrect 
-- we can ignore/delete these 9 records
-- vendorid wise analysis of these records
select vendorid , count(*) from main_data_ny
where ratecodeid = 99
group by vendorid;
-- 1 8
-- 2 1
-- Vendor 1 is the mazor contributor to this faulty data

-- ## store_and_fwd_flag
select store_and_fwd_flag, count(*) from main_data_ny group by store_and_fwd_flag;
-- the value of yes and no are fine

-- ## fare_amount
select max(fare_amount) max_fare_amount, min(fare_amount) min_fare_amount from main_data_ny;
-- max_fare_amount :- 650, min_fare_amount :- -200

select percentile_approx(fare_amount,array(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99)) from main_data_ny;
-- Result [4.936277914662287,5.952247700797058,6.92251039282117,7.970874767979148,9.390659940336155,10.927988505747127,12.998791213590113,16.812362001498627,24.8522171486555,51.958303868698714]
-- these values are acceptable let's look at smaller percentile value

select percentile_approx(fare_amount,array(0.01,0.999)) from main_data_ny;
-- Result [3.261611065392163,88.10268000000157]
-- even these values are within range
-- It seems like the negative values and very high values are wrong data or outliner.
select count(*) from main_data_ny where fare_amount<0;

-- 558 Records have negative values. we can ignore/delete these values

--  lets find a upper limit for these fare values
select count(*) from main_data_ny where fare_amount>100;
-- 770 Records have values greater than 100.
-- Let's check for more higher values
select count(*) from main_data_ny where fare_amount>300;
-- 15 Records have values greater than 300. We don't have information about the business and might be these fare prices indicate some intra city travel.
-- But without proper information we will ignore/delete these values

-- Vendor wise analysis
select vendorid ,count(*) from main_data_ny
where fare_amount> 300 or fare_amount<0  group by vendorid;
-- 1 6
-- 2 567
-- Vendor 2 is the major portion of the corrupt data

-- Extra :- Miscellaneous extras and surcharges. 

select max(extra) max_extra, min(extra) min_extra from main_data_ny;
-- max_extra :- 4.8, min_extra :- -10.6
-- But we know that extra only includes the $0.50 and $1 rush hour and overnight charges.
-- hence we will reject these values let's verify their count
select count(*) from main_data_ny where extra not in (0,0.5,1);
-- 4856 records are corrupt/faulty and can be ignored
-- Let's look it venderwise
select vendorid,count(*) from main_data_ny where extra not in (0,0.5,1) group by vendorid;
--  1 1823
--	2 3033
-- Data from vender 2 is more corrupt as compared to vender 1

-- mta_tax 
select max(mta_tax) max_mta_tax, min(mta_tax) min_mta_tax from main_data_ny;
-- max_mta_tax :- 11.4, min_mta_tax :- -0.5
-- we know that 0.50 MTA tax is automatically triggered based on the metered rate in use.
select count(*) from  maxim_assignment.Base_Data_Maxim BDM where mta_tax not in (0,0.5);
-- 548 records have incorrect data. we can ignore these
-- Venderwise Analysis
select vendorid,count(*) from main_data_ny where mta_tax not in (0,0.5) group by vendorid;
-- Both vendor are equally responsible, 
--  1 1
--	2 547
-- Data of Vendor 2 is faulty as only record is corrupt in vender 1 Data

-- tip_amount:- – This field is automatically populated for credit card tips. Cash tips are not included
select max(tip_amount) max_tip_amount, min(tip_amount) min_tip_amount from main_data_ny;
-- max_tip_amount :- 450, min_tip_amount :- -1.16
select count(*) from main_data_ny where tip_amount <0;
-- only 4 values are negative can be easily be ignored
-- Venderwise analysis
select vendorid,count(*) from main_data_ny where tip_amount <0 group by vendorid;
-- all belong to vendor 2

-- Let's chek if their are non credit card based tips
select count(*) from main_data_ny where Payment_type!=1 and tip_amount>0;
-- 17 records have payment mode other than credit and still have tip amount greate than 0

-- Venderwise Analysis
select vendorid,count(*) from main_data_ny where Payment_type!=1 and tip_amount>0  group by vendorid;
-- All these records belongs to vender 1
-- we will ignore these records as well.

-- tolls_amount :- Total amount of all tolls paid in trip. 
select max(tolls_amount ) max_tolls_amount, min(tolls_amount) min_tolls_amount from main_data_ny;
-- max_tolls_amount :- 895.89, min_tolls_amount :- -5.76
-- The value can't be negative
select count(*) from main_data_ny where tolls_amount <0;
-- only 3 records can be safely ignored
-- Venderwise analysis
select vendorid,count(*) from main_data_ny where tolls_amount <0
group by vendorid;
-- All records belongs to vendor 2

-- improvement_surcharge :- $0.30 improvement surcharge assessed trips at the flag drop
select max(improvement_surcharge) max_improvement_surcharge, min(improvement_surcharge) min_improvement_surcharge
from main_data_ny;
-- max_improvement_surcharge :- 1, min_improvement_surcharge :- -0.3
select count(*) from main_data_ny where improvement_surcharge not in (0,0.3);
-- only 562 records have corrupt data and can be easily ignored
-- vendorwise Analysis
select vendorid,count(*) from main_data_ny where improvement_surcharge not in (0,0.3) 
group by vendorid;
-- All records belong to vendor 2

-- ## total_amount :- max(total_amount ) max_total_amount, min(total_amount) min_total_amount	
select max(total_amount ) max_total_amount, min(total_amount) min_total_amount from main_data_ny;
-- max_total_amount :- 928.19, min_total_amount :- -200.8

-- Negative values
select count(*) from main_data_ny where total_amount<0;

-- 558 records have values less than 0 and can be ignored

-- Higehr value check
select * from main_data_ny where total_amount > 500;
-- 2 records have values greater than 500. one of them have 895.89 paid in toll and another one have the same pick up and drop.
-- We don't have business information and can ignore these 2 high value records.

-- vendorwise analysis
select vendorid,count(*) from main_data_ny where total_amount > 500 or total_amount<0 group by vendorid;
-- Group 2 have 558 incorrect records while vender 1 have only 2 incorrect records

-- # Basic Data Quality Checks
-- 3.You might have encountered unusual or erroneous rows in the dataset. 
-- Can you conclude which vendor is doing a bad job in providing the records using different columns of the dataset? Summarise your conclusions based on every column where these errors are present.

-- For the data It's mostly vendor 2 that is providing faulty data.
-- Below is the list of problemtic data they have provided column wise
-- invalid values for 1.total_amount, 2.improvement_surcharge, 3.tolls_amount, 4.tip_amount, 5.mta_tax, 
--  6.fare_amount, 7.passenger_count, 8.pickup and 9.drop off time
-- For the column extra both vendor seems to be equally at fault
-- Vendor 1 has few tip amount where the payment mode is not credit card
-- But overall Vendor 2 is definately not providing correct data
-- column wise analysis has already done.

-- ----------


-- Before answering the below questions, you need to create a clean, ORC partitioned table for analysis.
-- Remove all the erroneous rows.
    
    -- Creating the new orc table
    -- We will be partinening on the month column first as we need to answer question comparing between the two
    -- since we expect only two month data to pass from out filters year is not an any use in partionening
    
    -- Our secondarly partition is based on Vendor
    
    Create external table if not exists main_data_ny_orc_1(
    tpep_pickup_datetime timestamp,
    tpep_dropoff_datetime timestamp,
    passenger_count int,
    trip_distance double,
    RatecodeID int,
    store_and_fwd_flag string,
    PULocationID int,
    DOLocationID int,
    payment_type int,
    fare_amount double,
    extra double,
    mta_tax double,
    tip_amount double,
    tolls_amount double,
    improvement_surcharge double,
    total_amount double
    )
    partitioned by (Mnth int,VendorID int)
    stored as orc location '/user/sanjeevy94_gmail/ny_assignment_taxi'
    tblproperties ("orc.compress"="SNAPPY");
    -- ## Environment setup for partition was done in the starting of the file.


-- ## Inserting data
insert overwrite table main_data_ny_orc_1 partition(Mnth,VendorID)
    select tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count,
    trip_distance, RatecodeID, store_and_fwd_flag, PULocationID,
    DOLocationID, payment_type, fare_amount, extra, mta_tax,
    tip_amount, tolls_amount, improvement_surcharge,
    total_amount, month(tpep_pickup_datetime) Mnth,
    VendorID
    from main_data_ny
    where  (tpep_pickup_datetime >='2017-11-1 00:00:00.0' and tpep_pickup_datetime<'2018-01-01 00:00:00.0') and
    (tpep_dropoff_datetime >= '2017-11-1 00:00:00.0' and tpep_dropoff_datetime<'2018-01-02 00:00:00.0') and
    (tpep_dropoff_datetime > tpep_pickup_datetime) and
    (passenger_count not in (0)) and
    (trip_distance > 0) and 
    (ratecodeid!=99) and
    (fare_amount<= 300 and fare_amount > 0 ) and
     (extra in (0,0.5,1)) and
     (mta_tax in (0,0.5)) and 
    ((tip_amount >=0 and Payment_type=1) or (Payment_type!=1 and tip_amount=0)) and
    ( tolls_amount >=0) and
    ( improvement_surcharge in (0,0.3)) and
    (total_amount<= 500 and total_amount > 0 );

-- # Data Check
    select count(*) from main_data_ny_orc_1;
    -- 1153579
    select 1174568-1153579;
    -- 20989 were removed
    select 20989/1174568*100;
    -- amounting to 1.78% of data is removed

-- # Analysis-I

    -- 1 .Compare the overall average fare per trip for November and December
        select mnth,round(avg(total_amount),2) Avg_total_amt,round(avg(fare_amount),2) Avg_fare_amount
        from main_data_ny_orc_1 group by mnth;
        -- Month 11 :- Avg_total_amt :- 16.19 and Avg_fare_amount :- 12.9
        -- Month 12 :- Avg_total_amt :- 15.89 and Avg_fare_amount :- 12.7
        
        select (16.19-15.89) as Nov, (12.9-12.7) as dec;
        
        -- Overall the month Novemeber seems to be better considering total amount.
        -- Also the difference in fare amount avg is on the lower side when compared to total amount
        -- this signifies that extra tax and charges are also coming in play during the month of November
        
    -- 2. Explore the ‘number of passengers per trip’ - how many trips are made by each level of ‘Passenger_count’? 
    -- Do most people travel solo or with other people?
        select passenger_count,round((count(*)*100/1153579),2) counts from main_data_ny_orc_1 group by passenger_count
        order by counts desc;
        --  1	70.82
        --	2	15.15
        --	5	4.68
        --	3	4.35
        --	6	2.85
        --	4	2.14
        -- Solo rides are most common , dominant infact with almost 71% od data belonging to them
        -- Dual rides are the only other significant category with 15% occupancy
        -- Rest all are below 5 %
        -- value for 7 is too small to be of any significance, may be special limo rides, corrupt data
        select passenger_count,count(*) counts from main_data_ny_orc_1 where passenger_count in (9,8,7) 
        group by passenger_count order by counts desc;
        -- 7	3
    
    -- 3.Which is the most preferred mode of payment?
        select payment_type,round((count(*)*100/1153579),2) counts from main_data_ny_orc_1 group by payment_type
        order by counts desc;
        --1	67.54 Credit card
        --2	31.96 Cash
        --3	0.39  No charge
        --4	0.11  Dispute
        -- Credit card pays are dominant with 67.5% and cash payment are 2nd highest paymnet approx 32%
        -- rest all modes are negligable
        -- 5 & 6 are not existance as previsously seen.
        
    -- 4.What is the average tip paid per trip? 
-- Compare the average tip with the 25th, 50th and 75th percentiles and 
-- comment whether the ‘average tip’ is a representative statistic (of the central tendency) of ‘tip amount paid’. 
-- Hint: You may use percentile_approx(DOUBLE col, p): 
-- Returns an approximate pth percentile of a numeric column (including floating point types) in the group.
        select round(avg(tip_amount),2) from main_data_ny_orc_1;
        -- 1.83
        
        select percentile_approx(tip_amount,array(0.25,0.40,0.45,0.50,0.60,0.65,0.75))  
        from main_data_ny_orc_1;
        
        --   25%, 0.40, 0.45, 50%,  60%,  65%,  75%
        -- 	[0.0, 0.99 , 1.14, 1.36, 1.76, 1.99, 2.45]
        -- From the %centile values we can see that data is skewed towards teh higher side.
        -- 25% or more values bein zero tip do play a high part in this behaviour
        -- again the median 1.36 is much lower then the avg 1.82 due to the skewness towards higher values
        -- Hence mean is not representative statistic of centeral tendency here.
    
    -- 5. Explore the ‘Extra’ (charge) variable - what fraction of total trips have an extra charge is levied?
        select extra,round((count(*)*100/1153579),2) counts from (select case when extra > 0 then 1 else 0 end
        extra from main_data_ny_orc_1 ) new group by extra order by counts desc;
        -- Extra applied    %age records
        --      0	        53.85
        --      1	        46.15
        -- The distribusion is fairly even with 46.15% records having extra charges applied, where as  53.87% have no extra charges applied
        

-- Analysis-II 

    -- 1. What is the correlation between the number of passengers on any given trip, and the tip paid per trip? 
    --    Do multiple travellers tip more compared to solo travellers? Hint: Use CORR(Col_1, Col_2)
        select round(corr(passenger_count, tip_amount),4) from main_data_ny_orc_1;
        -- -0.0053
        -- the value is fairly small although negative but its would be fair to say that passenger count is unrealted to the tip amount paid.
        select round(corr(is_solo, tip_amount),4) from (select case when passenger_count=1 then 1 else 0 end
        is_solo, tip_amount from main_data_ny_orc_1) new;
        -- 0.0062, comparing only single vs multiple rider count their is still very low co-relation
        select is_solo,round(avg(tip_amount),4) from (select case when passenger_count=1 then 1 else 0 end
        is_solo, tip_amount from main_data_ny_orc_1) new group by is_solo;
        --  0	1.8022
        --	1	1.8354
        -- Values are almost same 


    -- 2. Segregate the data into five segments of ‘tip paid’: [0-5), [5-10), [10-15) , [15-20) and >=20. 
    --    Calculate the percentage share of each bucket (i.e. the fraction of trips falling in each bucket).
        select Tip_range, round((count(*)*100/1153579),2) counts from (select
        case when (tip_amount>=0 and tip_amount<5)   then '[0-5)' 
             when (tip_amount>=5 and tip_amount<10)  then '[5-10)' 
             when (tip_amount>=10 and tip_amount<15) then '[10-15)'
             when (tip_amount>=15 and tip_amount<20) then '[15-20)'
             when (tip_amount>=20) then '>=20' end Tip_range from main_data_ny_orc_1) new
             group by Tip_range order by counts desc;
        -- [0-5)	92.4
        -- [5-10)	5.64
        -- [10-15)	1.68
        -- [15-20)	0.19
        -- >=20	    0.09
        -- 0-5 range is the most prominate group with 92.4% records, we already know 25%+ of these are 0 values from the previous percentile based check
        -- 5-10 represening a small fraction of 5.6%, remaning set are almost negligible amount to 2% of data
        
    -- 3.Which month has a greater average ‘speed’ - November or December?
    -- Note that the variable ‘speed’ will have to be derived from other metrics.
    -- Hint: You have columns for distance and time.

        -- we will calculate duration by subtracting drop of time with pick up time. we are using unix timestamp function(as direct suntraction of timestamp column didn't work)
        -- values will be returned in sec hence we will be dividing it by 3600 to get values in hour
        -- since distance is specified in miles our final value will be in miles/hour unit
        
        select mnth , round(avg(trip_distance/((unix_timestamp(tpep_dropoff_datetime)-unix_timestamp(tpep_pickup_datetime) )/3600) ),2) avg_speed
        from main_data_ny_orc_1 group by mnth order by avg_speed desc;
        --  11	10.97
        --	12	11.07
        -- December month is marginally faster by 0.1 miles/hour
        
    
    -- 4.Analyse the average speed of the most happening days of the year
    -- i.e. 31st December (New year’s eve) and 25th December (Christmas Eve) and compare it with the overall average. 
    
        -- any trip that started on 25th or 31 will be considerd for the avg calculation irrespective of the fact that it might have ended on the next day
        select IsHoliday, round(avg(speed),2) avg_speed from (select case when 
        ((tpep_pickup_datetime >= '2017-12-25 00:00:00.0' and tpep_pickup_datetime < '2017-12-26 00:00:00.0') 
        or (tpep_pickup_datetime >= '2017-12-31 00:00:00.0' and tpep_pickup_datetime < '2018-01-01 00:00:00.0')  ) then 1 else 0 end IsHoliday, 
        trip_distance/((unix_timestamp(tpep_dropoff_datetime)-unix_timestamp(tpep_pickup_datetime) )/3600) speed
        from main_data_ny_orc_1) new group by IsHoliday order by avg_speed desc;
        -- 1	14.01
        -- 0	10.95
        select 14.01-10.95;
        -- The comparision between holiday vs non-holiday shows that during the holiday the streets of New York are clear
        -- as the Cab's are running at a faster average speed by a margin of  3.06 miles/hour
        -- The non festive day average is in sync with november and december averages at around 10.95 miles/per hour
        -- let's confirm the overall averages once
        select round(avg(trip_distance/((unix_timestamp(tpep_dropoff_datetime)-unix_timestamp(tpep_pickup_datetime) )/3600)),2) avg_speed from main_data_ny_orc_1;
        -- 11.02 is the overall avg speed as expected so the faster speed on 25th and 31 dec amounts to 0.05(12.61 was for non holiday days) increment on the overall speed 
        
        -- Let's compare individual days too
        -- christmas
        select Day_type,round(avg(trip_distance/((unix_timestamp(tpep_dropoff_datetime)-unix_timestamp(tpep_pickup_datetime) )/3600)),2) avg_speed
        from (select trip_distance,tpep_dropoff_datetime,tpep_pickup_datetime,
        case when ((tpep_pickup_datetime>='2017-12-25 00:00:00.0' and tpep_pickup_datetime<'2017-12-26 00:00:00.0')) then 1
        when ((tpep_pickup_datetime>='2017-12-31 00:00:00.0' and tpep_pickup_datetime<'2018-01-01 00:00:00.0')  ) then 2 else 0 end Day_type 
        from main_data_ny_orc_1) new group by Day_type;
        
        --  0	10.95 rest of the days
        --	1	15.27 Christmas
        --  2	13.24 new year eve
        -- The fasted avg speed is observed on Christmas day @ 15.27 miles/hour; 2.03 miles/hour faster than new year eve mark of 13.24 miles/hour
        -- The result represent similar value to the combined holiday data i.e. Both are individually much faster than the average time taken on other days

